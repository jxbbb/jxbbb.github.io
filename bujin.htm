<!DOCTYPE html>
<html lang="en"><head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Bu Jin - Homepage</title>
    
    <meta name="author" content="Bu Jin">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <link rel="stylesheet" type="text/css" href="files/stylesheet.css">
<script type="text/javascript" src="files/jquery.min.js"></script></head>

<body data-new-gr-c-s-check-loaded="8.912.0" data-gr-ext-installed="">
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Bu Jin (晋步)</name>
                </p>
                <p style="text-align:center">
                  <a href="jinbu18@mails.ucas.ac.cn"><b>Email</b></a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=uUd5v2cAAAAJ&hl=en"><b>Google Scholar</b></a> &nbsp;/&nbsp;
                  <a href="https://github.com/jxbbb/"><b>GitHub</b></a>
              </p>
                <p style="text-align:justify;line-height:180%">
                  I am currently a master student in Institute of Automation, Chinese Academy of Sciences, advised by <a href="https://scholar.google.com/citations?user=sOI-S7oAAAAJ&hl=en&oi=ao">Prof. Jing Liu</a>. Before that, I obtained the bachelor degree from University of Chinese Academy of Sciences. Currently, I am also a research intern at <a href="https://air.tsinghua.edu.cn/"><b>Tsinghua AIR</b></a>, focusing on perception and planning in autonomous driving. I am fortunate to work closely with <a href="https://www.xxlong.site/">Dr. Xiaoxiao Long</a> from HKU and Prof. <a href="https://sites.google.com/view/fromandto">Prof. Hao Zhao</a> from Tsinghua AIR DISCOVER Lab.
		            </p>

                  <p>
                  <b><strong>I am actively looking for the PhD opportunity in Fall 2025.</strong></b>
                  </p>

              </td>
              <td style="padding:2.5%;width:48%;max-width:48%">
                <img style="width:80%;max-width:80%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="files/avator.png" class="hoverZoomLink">
              </td>
	      </tr>
	      </tbody></table>

<!--  ------------------------ Research ------------------------------>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;line-height:180%"><tbody>
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
    <heading>Research</heading>
    <p style="text-align:justify;">
      My research interests lie in Autonomous Driving and Generative Models, and I'm currently focusing on World Models in Autonomous Driving:
    </p>
    <p>
      </p>
      <li style="margin: 5px;"> 
        🚗 <strong>Autonomous driving:</strong> LLM Driver; Interpretable Autonomous Driving; World Model.
      </li>
      <li style="margin: 5px;"> 
        👀 <strong>3D Vision:</strong> 3D Scene Understanding; 3D Generation.
      </li>

    <p></p>
  </td>
</tr>

<!--  ----------------------- PUBLICATIONS --------------------------  -->

        </tbody></table><table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              (<sup>†</sup>: corresponding author; <sup>*</sup>: equal contribution)
            </td>
          </tr>
<!-- 	(<sup>†</sup>: corresponding author; <sup>*</sup>: equal contribution) -->
        </tbody></table>	
        
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;line-height:180%"><tbody>
            <tr>
              <td style="padding:14px;width:45%;max-width:45%" align="center">
                  <img style="width:100%;max-width:100%" src="files/tod3cap.png" alt="dise">
              </td>
              <td width="75%" valign="center">
                <a href="https://jxbbb.github.io/TOD3Cap"><papertitle>TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes</papertitle></a>
                <br>
                <strong>Bu Jin</strong>,
                  Yupeng Zheng<sup>†</sup>,
                  Pengfei Li,
                  Weize Li,
                  Yuhang Zheng,
                  Sujie Hu,
                  Xinyu Liu,
                  Jinwei Zhu,
                  Zhijie Yan,
                  Haiyang Sun,
                  Kun Zhan,
                  Peng Jia,
                  Xiaoxiao Long,
                  Yilun Chen,
                  Hao Zhao
                <br>
                <strong><em>ECCV, 2024</em></strong>
                <br>
                <a href="https://jxbbb.github.io/TOD3Cap"><b>[Homepage]</b></a>
                <a href="https://arxiv.org/abs/2403.19589"><b>[arXiv]</b></a>
                <a href="https://github.com/jxbbb/TOD3Cap"><b>[Code]</b></a>
                <a href="https://github.com/jxbbb/TOD3Cap"><img src="https://img.shields.io/github/stars/jxbbb/TOD3Cap.svg?style=social&label=Star&maxAge=2592000"></a>
                <br>
                <!-- <p>We introduced the new task of outdoor 3D dense captioning with TOD3Cap dataset; We proposed TOD3Cap network, leveraging the BEV representation to encode sparse outdoor scenes, and combine Relation Q-Former with LLaMA-Adapter to dense captioning in the open-world. </p>	 -->
              </td>
            </tr>

            <tr>
              <td style="padding:14px;width:45%;max-width:45%" align="center">
                  <img style="width:100%;max-width:100%" src="files/adapt.gif" alt="dise">
              </td>
              <td width="75%" valign="center">
                <a href="https://ieeexplore.ieee.org/abstract/document/10160326"><papertitle>Adapt: Action-aware Driving Caption Transformer</papertitle></a>
                <br>
                <strong>Bu Jin</strong>, 
                  Xinyu Liu, 
                  Yupeng Zheng, 
                  Pengfei Li, 
                  Hao Zhao<sup>†</sup>, 
                  Tong Zhang, 
                  Yuhang Zheng, 
                  Guyue Zhou, 
                  Jingjing Liu
                <br>
                <strong><em>ICRA, 2023 </em></strong>
                <br>
                <a href="https://arxiv.org/abs/2302.00673"><b>[arXiv]</b></a>
                <a href="https://github.com/jxbbb/ADAPT"><b>[Code]</b></a>
                <a href="https://github.com/jxbbb/ADAPT"><img src="https://img.shields.io/github/stars/jxbbb/ADAPT.svg?style=social&label=Star&maxAge=2592000"></a>

                <br>
                <!-- <p>We presented Adapt (Action-aware Driving cAPtion Transformer), a new end-to-end transformer-based framework for generating action narration and reasoning for self-driving vehicle. </p>	 -->
              </td>
            </tr>

            <tr>
              <td style="padding:14px;width:45%;max-width:45%" align="center">
                  <img style="width:100%;max-width:100%" src="files/lasst.png" alt="dise">
              </td>
              <td width="75%" valign="center">
                <a href="https://dl.acm.org/doi/abs/10.1145/3552482.3556555"><papertitle>Language-guided Semantic Style Transfer of 3D Indoor Scenes</papertitle></a>
                <br>
                <strong>Bu Jin</strong>, 
                Beiwen Tian,
                Hao Zhao<sup>†</sup>,
                Guyue Zhou
                <br>
                <strong><em>PIES-ME oral, 2022 </em></strong>
                <br>
                <a href="https://arxiv.org/abs/2208.07870"><b>[arXiv]</b></a>
                <a href="https://github.com/AIR-DISCOVER/LASST"><b>[Code]</b></a>
                <a href="https://github.com/AIR-DISCOVER/LASST"><img src="https://img.shields.io/github/stars/AIR-DISCOVER/LASST.svg?style=social&label=Star&maxAge=2592000"></a>

                <br>
                <!-- <p>We presented Adapt (Action-aware Driving cAPtion Transformer), a new end-to-end transformer-based framework for generating action narration and reasoning for self-driving vehicle. </p>	 -->
              </td>
            </tr>

          <tr>
            <td style="padding:14px;width:45%;max-width:45%" align="top">
                <img style="width:100%;max-width:100%" src="files/gaussiangrasper.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://mrsecant.github.io/GaussianGrasper/"><papertitle>GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping</papertitle></a>
              <br>
                Yuhang Zheng,
								Xiangyu Chen,
								Yupeng Zheng,
		            Songen Gu,
                Runyi Yang,
                <strong>Bu Jin</strong>,
                Pengfei Li,
                Chengliang Zhong,
                Zengmao Wang,
                Lina Liu,
                Chao Yang,
                Dawei Wang,
                Zhen Chen,
		            Xiaoxiao Long<sup>†</sup>,
		            Meiqing Wang<sup>†</sup>.
              <br>
              <strong><em>RA-L, 2024</em></strong>
              <br>
              <a href="https://mrsecant.github.io/GaussianGrasper"><b>[Homepage]</b></a>
              <a href="https://arxiv.org/abs/2403.09637"><b>[arXiv]</b></a>
	            <a href="https://github.com/MrSecant/GaussianGrasper"><b>[Code]</b></a>
              <a href="https://github.com/MrSecant/GaussianGrasper"><img src="https://img.shields.io/github/stars/MrSecant/GaussianGrasper.svg?style=social&label=Star&maxAge=2592000"></a>
	            <br>
	            <!-- <p>We introduced GaussianGrasper, a robot grasping system implemented by a 3D Gaussian field endowed with open-vocabulary semantics and accurate geometry that is capable of rapid updates to support open-world robotic grasping guided by language.</p>	 -->
            </td>
          </tr>



          <tr>
            <td style="padding:14px;width:45%;max-width:45%" align="center">
                <img style="width:100%;max-width:100%" src="files/monoocc.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2403.08766"><papertitle>MonoOcc: Digging into Monocular Semantic Occupancy Prediction</papertitle></a>
              <br>
                Yupeng Zheng<sup>*</sup>,
                Xiang Li<sup>*</sup>,
								Pengfei Li,
                Yuhang Zheng,
                <strong>Bu Jin</strong>,
		            Chengliang Zhong,
                Xiaoxiao Long,
                Hao Zhao,
                Qichao Zhang<sup>†</sup>
              <br>
              <strong><em>ICRA, 2024</em></strong>
              <br>
              <a href="https://arxiv.org/abs/2403.08766"><b>[arXiv]</b></a>
	            <a href="https://github.com/ucaszyp/MonoOcc"><b>[Code]</b></a>
              <a href="https://github.com/ucaszyp/MonoOcc"><img src="https://img.shields.io/github/stars/ucaszyp/MonoOcc.svg?style=social&label=Star&maxAge=2592000"></a>

	            <br>
	            <!-- <p>We presented MonoOcc, a high-performance and efficient framework for monocular semantic occupancy prediction. We (1) propose an auxiliary semantic loss as supervision and an image-conditioned cross-attention module to refine voxel feature, and (2) employ a distillation module to transfer richer knowledge.  </p>	 -->
            </td>
          </tr>


          <tr>
            <td style="padding:14px;width:45%;max-width:45%" align="center">
                <img style="width:100%;max-width:100%" src="files/steps.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://ieeexplore.ieee.org/abstract/document/10160708/"><papertitle>Steps: Joint Self-supervised Nighttime Image Enhancement and Depth Estimation</papertitle></a>
              <br>
                Yupeng Zheng, 
                Chengliang Zhong, 
                Pengfei Li, 
                Huan-ang Gao, 
                Yuhang Zheng, 
                <strong>Bu Jin</strong>, 
                Ling Wang, 
                Hao Zhao, 
                Guyue Zhou, 
                Qichao Zhang, 
                Dongbin Zhao<sup>†</sup>
              <br>
              <strong><em>ICRA, 2023 </em></strong>
              <br>
              <a href="https://arxiv.org/abs/2302.01334"><b>[arXiv]</b></a>
	            <a href="https://github.com/ucaszyp/STEPS"><b>[Code]</b></a>
              <a href="https://github.com/ucaszyp/STEPS"><img src="https://img.shields.io/github/stars/ucaszyp/STEPS.svg?style=social&label=Star&maxAge=2592000"></a>

	            <br>
	            <!-- <p>We presented STEPS, the first method that jointly learns a nighttime image enhancer and a depth estimator with a self-supervised manner. And a newly proposed uncertain pixel masking strategy is used to tightly entangle these two task. </p>	 -->
            </td>
          </tr>
        </tbody></table>
		
<!--  ----------------------- PUBLICATIONS --------------------------  -->

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      <heading>In Submission</heading>
      (<sup>†</sup>: corresponding author; <sup>*</sup>: equal contribution)  
    </td>
  </tr>
<!-- 	(<sup>†</sup>: corresponding author; <sup>*</sup>: equal contribution) -->
</tbody>
</table>	

 
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;line-height:180%"><tbody>
  <tr>
    <td style="padding:14px;width:45%;max-width:45%" align="center">
        <img style="width:100%;max-width:100%" src="files/planagent.png" alt="dise">
    </td>
    <td width="75%" valign="center">
      <a href="https://arxiv.org/abs/2406.01587"><papertitle>PlanAgent: A Multi-modal Large Language Agent for Closed-loop Vehicle Motion Planning</papertitle></a>
      <br>
        Yupeng Zheng<sup>*</sup>, 
        Zebin Xing<sup>*</sup>,
        Qichao Zhang<sup>†</sup>, 
        <strong>Bu Jin</strong>, 
        Pengfei Li, 
        Yuhang Zheng,
        Zhongpu Xia, 
        Kun Zhan, 
        Xianpeng Lang, 
        Yaran Chen, 
        Dongbin Zhao 
      <br>
      <strong><em>TITS, in submission</em></strong>
      <br>
      <a href="https://arxiv.org/abs/2406.01587"><b>[arXiv]</b></a>
      <br>
      <!-- <p>We proposed PlanAgent, the first mid-to-mid planning system based on a Multi-modal Large Language Model (MLLM), which is used as a cognitive agent to introduce human-like knowledge, interpretability, and commonsense reasoning into the closed-loop planning.</p>	 -->
    </td>
  </tr>       


        <!--  ------------------------ MISC -------------------------- -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Others</heading>
            <p style="text-align:justify;">
              Outside of research, I enjoy swimming🏊 and traveling🚢.
            </p>
          </td>
        </tr>


		
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>

            </td>
          </tr>
        </tbody></table>

        </td>
      </tr>
    </tbody></table>
   
    <p></p><center>
            <div id="clustrmaps-widget" style="width:6%">
            </div>        
            <br>
            © Bu Jin | Last update: July.24, 2024
            <p style="text-align:right;">
              Website template from <a href="https://jonbarron.info/">Jon Barron</a>.
            </p>
    </center><p></p>
  

</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>